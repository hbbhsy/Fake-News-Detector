{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!nohub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nohup: ignoring input and appending output to ‘nohup.out’\n",
      "nohup: ignoring input and appending output to ‘nohup.out’\n"
     ]
    }
   ],
   "source": [
    "!nohup pip install wrapt --upgrade --ignore-installed\n",
    "!nohup pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Recall, Precision\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Testing and optimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# import module\n",
    "from pipeline import *\n",
    "from model import *\n",
    "\n",
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "import pickle\n",
    "import datetime as dt\n",
    "import glob\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv from s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = 'fakenewscorpus'\n",
    "key = 'data/news_cleaned_2018_02_13.csv'\n",
    "df = pd.read_csv('s3://{}/{}'.format(bucket,key), engine = 'python', nrows = 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = 'fakenewscorpus'\n",
    "key = 'data/5M_df.pkl'\n",
    "# obj = s3.get_object(Bucket='bucket', Key='key')\n",
    "df = pickle.loads(s3.Bucket(bucket).Object(key).get()['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33273061724603664"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance(df):\n",
    "    n_pos = len(df[df['label']==1])\n",
    "    n_neg = len(df[df['label']==0])\n",
    "    return n_pos, n_neg\n",
    "\n",
    "n_pos, n_neg = balance(df)\n",
    "\n",
    "n_pos/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33258, 66742)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = pd.read_csv('data/sw1k.csv')['term'].to_numpy()\n",
    "\n",
    "sample = df.sample(100000,axis=0)\n",
    "\n",
    "balance(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(sample['content'],sw)\n",
    "\n",
    "sample['token']=tokens\n",
    "\n",
    "X = tokens\n",
    "y = sample['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "bow, tf, tfidf, cv, tv = vectorize(X_train,max_features=5000,ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf).to_csv('data/sample_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('data/sample_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 5000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5466"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGHT = len(max(X_train, key=len))\n",
    "MAX_SEQ_LENGHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'fsu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3f157584ab1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN_FEATURES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mvaries\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfastest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mFORTRAN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontiguous\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mvaries\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfastest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'fsu'"
     ]
    }
   ],
   "source": [
    "N_FEATURES = len(bow)\n",
    "X_train_sequences = pad_sequences(X_train, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences :  [[22], [3], [2], [1], [], [3], [5], [12], [], [7], [14], [7], [3], [5], [5], [3], [], [2], [4], [4], [22], [], [3], [], [15], [4], [15], [], [3], [2], [], [2], [3], [17], [11], [4], [8], [], [3], [5], [12], [], [2], [4], [13], [], [], [16], [3], [8], [8], [3], [20], [3], [17], [], [20], [3], [7], [], [8], [1], [15], [4], [8], [2], [6], [5], [16], [], [2], [9], [1], [], [5], [1], [20], [7], [], [], [3], [5], [5], [4], [14], [5], [10], [6], [5], [16], [], [2], [9], [3], [2], [], [7], [20], [6], [18], [2], [], [9], [3], [7], [], [19], [1], [1], [5], [], [14], [5], [18], [4], [11], [11], [4], [20], [1], [12], [], [19], [17], [], [9], [1], [8], [], [1], [28], [], [19], [4], [17], [18], [8], [6], [1], [5], [12], [], [10], [3], [11], [21], [6], [5], [], [9], [3], [8], [8], [6], [7], [], [4], [5], [], [2], [20], [6], [2], [2], [1], [8], [], [], [], [8], [1], [15], [4], [8], [2], [7], [], [9], [3], [21], [1], [], [7], [14], [8], [18], [3], [10], [1], [12], [], [4], [21], [1], [8], [5], [6], [16], [9], [2], [], [2], [9], [3], [2], [], [2], [9], [1], [], [15], [4], [15], [], [7], [2], [3], [8], [7], [], [], [], [20], [9], [4], [], [3], [5], [5], [4], [14], [5], [10], [1], [12], [], [2], [9], [1], [6], [8], [], [19], [8], [1], [3], [22], [], [14], [15], [], [3], [5], [12], [], [6], [5], [7], [6], [7], [2], [1], [12], [], [6], [2], [], [20], [3], [7], [], [3], [13], [6], [10], [3], [19], [11], [1], [], [11], [3], [7], [2], [], [20], [1], [1], [22], [], [], [], [9], [3], [21], [1], [], [18], [3], [11], [11], [1], [5], [], [4], [14], [2], [], [19], [1], [10], [3], [14], [7], [1], [], [7], [20], [6], [18], [2], [], [9], [3], [7], [], [19], [1], [1], [5], [], [10], [3], [14], [16], [9], [2], [], [10], [4], [7], [17], [6], [5], [16], [], [14], [15], [], [2], [4], [], [2], [9], [1], [], [5], [6], [16], [9], [2], [], [13], [3], [5], [3], [16], [1], [8], [], [3], [10], [2], [4], [8], [], [3], [5], [12], [], [24], [3], [13], [1], [7], [], [19], [4], [5], [12], [], [9], [4], [15], [1], [18], [14], [11], [], [2], [4], [13], [], [9], [6], [12], [12], [11], [1], [7], [2], [4], [5], [], [], [], [2], [9], [1], [8], [1], [], [9], [3], [7], [], [19], [1], [1], [5], [], [5], [4], [], [10], [4], [5], [18], [6], [8], [13], [3], [2], [6], [4], [5], [], [3], [7], [], [4], [18], [], [17], [1], [2], [], [], [19], [14], [2], [], [16], [3], [8], [8], [3], [20], [3], [17], [], [20], [3], [7], [], [35], [14], [6], [10], [22], [], [2], [4], [], [16], [6], [21], [1], [], [9], [1], [8], [], [4], [15], [6], [5], [6], [4], [5], [], [], [], [36], [7], [9], [1], [], [], [7], [20], [6], [18], [2], [], [], [6], [7], [5], [27], [2], [], [16], [4], [6], [5], [16], [], [2], [4], [], [19], [1], [], [7], [9], [4], [8], [2], [], [4], [18], [], [4], [18], [18], [1], [8], [7], [], [37], [], [2], [9], [1], [], [], [], [], [17], [1], [3], [8], [], [4], [11], [12], [], [15], [8], [1], [7], [1], [5], [2], [1], [8], [], [7], [3], [6], [12], [], [], [36], [20], [9], [17], [], [10], [3], [5], [27], [2], [], [7], [9], [1], [], [24], [14], [7], [2], [], [11], [1], [3], [21], [1], [], [2], [4], [13], [], [9], [6], [12], [12], [11], [1], [7], [2], [4], [5], [], [3], [11], [4], [5], [1], [], [18], [4], [8], [], [14], [7], [], [37]] \n",
      "\n",
      "word_index :  {'e': 1, 't': 2, 'a': 3, 'o': 4, 'n': 5, 'i': 6, 's': 7, 'r': 8, 'h': 9, 'c': 10, 'l': 11, 'd': 12, 'm': 13, 'u': 14, 'p': 15, 'g': 16, 'y': 17, 'f': 18, 'b': 19, 'w': 20, 'v': 21, 'k': 22, \"'\": 23, 'j': 24, '1': 25, '2': 26, '’': 27, 'x': 28, '3': 29, 'z': 30, '0': 31, '8': 32, '6': 33, '5': 34, 'q': 35, '“': 36, '”': 37, '…': 38, '‘': 39}\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer(num_words = 500)\n",
    "s = df['content']\n",
    "t.fit_on_texts(s)\n",
    "sq = t_temp.texts_to_sequences(r)\n",
    "# print('sequences : ', sq,'\\n')\n",
    "# print('word_index : ',t_temp.word_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have 'read' and 'readline' attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a8f15a0afe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/5M_df.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
     ]
    }
   ],
   "source": [
    "df = pickle.load('data/5M_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "y = df['label']\n",
    "t = Tokenizer(num_words=5000)\n",
    "t.fit_on_texts(df['content'])\n",
    "X = t.texts_to_sequences(df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = pad_sequences(X, maxlen=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_matrix,y)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239504, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             2576640   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,609,729\n",
      "Trainable params: 2,609,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential()\n",
    "\n",
    "lstm_model.add(LSTM(units=64, return_sequences=True, input_shape=(1,5000)))\n",
    "lstm_model.add(LSTM(64),)\n",
    "lstm_model.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC','BinaryAccuracy','Recall','Precision'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_history = lstm_model.fit(X_train, y_train, \n",
    "          epochs=50, batch_size=128, verbose=1,\n",
    "          validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save('../saved_model/sample_lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('saved_model/sample_model',compile = False)\n",
    "tfidf = pd.read_csv('data/sample_tfidf.csv')\n",
    "X_train = tfidf.set_ind\n",
    "y_train = sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8309/8309 [==============================] - 1s 79us/sample - loss: 0.6295 - auc: 0.6636 - binary_accuracy: 0.6077 - recall: 0.7783 - precision: 0.6135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6294791400324984, 0.66362023, 0.60765433, 0.7782875, 0.6134642]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(X_test, y_test, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = lstm_model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = pd.read_csv('../data/sw1k.csv')['term'].to_numpy()\n",
    "\n",
    "df['token']=tokenize(df['content'],sw)\n",
    "\n",
    "\n",
    "X = df['token']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "bow, tf, tfidf, cv, tv = vectorize(X_train,max_features=10000,ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [' '.join(row) for row in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 5,001,001\n",
      "Trainable params: 5,001,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    " \n",
    "model2.add(Dense(units=500, activation='relu', input_dim=10000))\n",
    "model2.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC','BinaryAccuracy','Recall','Precision'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21925 samples, validate on 3000 samples\n",
      "Epoch 1/3\n",
      "21925/21925 [==============================] - 1s 61us/sample - loss: 0.0015 - auc_2: 1.0000 - binary_accuracy: 0.9999 - recall_2: 0.9999 - precision_2: 0.9999 - val_loss: 0.5204 - val_auc_2: 0.9584 - val_binary_accuracy: 0.9293 - val_recall_2: 0.9297 - val_precision_2: 0.9401\n",
      "Epoch 2/3\n",
      "21925/21925 [==============================] - 1s 61us/sample - loss: 0.0049 - auc_2: 0.9999 - binary_accuracy: 0.9984 - recall_2: 0.9987 - precision_2: 0.9985 - val_loss: 0.6347 - val_auc_2: 0.9529 - val_binary_accuracy: 0.9290 - val_recall_2: 0.9224 - val_precision_2: 0.9461\n",
      "Epoch 3/3\n",
      "21925/21925 [==============================] - 1s 62us/sample - loss: 0.0851 - auc_2: 0.9948 - binary_accuracy: 0.9724 - recall_2: 0.9732 - precision_2: 0.9765 - val_loss: 0.2710 - val_auc_2: 0.9734 - val_binary_accuracy: 0.9280 - val_recall_2: 0.9462 - val_precision_2: 0.9237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6646c64f28>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(tfidf, y_train, \n",
    "          epochs=3, batch_size=128, verbose=1,\n",
    "          validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('../saved_model/sample_mlp_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8309/8309 [==============================] - 0s 59us/sample - loss: 2.7775 - auc_2: 0.5000 - binary_accuracy: 0.4514 - recall_2: 0.0000e+00 - precision_2: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.7775004928128384, 0.5, 0.4514382, 0.0, 0.0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143702/143702 [==============================] - 16s 114us/sample - loss: 0.0265 - auc_2: 0.9993 - binary_accuracy: 0.9936 - recall_2: 0.9824 - precision_2: 0.9967 - val_loss: 0.1983 - val_auc_2: 0.9704 - val_binary_accuracy: 0.9439 - val_recall_2: 0.8717 - val_precision_2: 0.9433\n",
      "Epoch 6/50\n",
      "143702/143702 [==============================] - 16s 114us/sample - loss: 0.0116 - auc_2: 0.9997 - binary_accuracy: 0.9979 - recall_2: 0.9942 - precision_2: 0.9989 - val_loss: 0.2201 - val_auc_2: 0.9676 - val_binary_accuracy: 0.9449 - val_recall_2: 0.8743 - val_precision_2: 0.9441\n",
      "Epoch 7/50\n",
      "143702/143702 [==============================] - 16s 114us/sample - loss: 0.0067 - auc_2: 0.9997 - binary_accuracy: 0.9990 - recall_2: 0.9977 - precision_2: 0.9990 - val_loss: 0.2435 - val_auc_2: 0.9653 - val_binary_accuracy: 0.9447 - val_recall_2: 0.8749 - val_precision_2: 0.9430\n",
      "Epoch 8/50\n",
      "143702/143702 [==============================] - 16s 115us/sample - loss: 0.0052 - auc_2: 0.9997 - binary_accuracy: 0.9993 - recall_2: 0.9986 - precision_2: 0.9992 - val_loss: 0.2640 - val_auc_2: 0.9623 - val_binary_accuracy: 0.9448 - val_recall_2: 0.8746 - val_precision_2: 0.9435\n",
      "Epoch 9/50\n",
      "143702/143702 [==============================] - 16s 114us/sample - loss: 0.0042 - auc_2: 0.9997 - binary_accuracy: 0.9995 - recall_2: 0.9990 - precision_2: 0.9993 - val_loss: 0.2737 - val_auc_2: 0.9624 - val_binary_accuracy: 0.9441 - val_recall_2: 0.8745 - val_precision_2: 0.9414\n",
      "Epoch 10/50\n",
      " 79360/143702 [===============>..............] - ETA: 6s - loss: 0.0037 - auc_2: 0.9997 - binary_accuracy: 0.9996 - recall_2: 0.9993 - precision_2: 0.9992"
     ]
    }
   ],
   "source": [
    "history = model2.fit(tfidf, y_train, \n",
    "          epochs=50, batch_size=128, verbose=1,\n",
    "          validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, metric):\n",
    "# Plot training & validation accuracy values\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_{}'.format(metric)])\n",
    "    plt.title('Model {}'.format(metric), size = 32)\n",
    "    plt.ylabel('{}'.format(metric))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "mlp_model = load_model('../saved_model/sample_mlp_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
