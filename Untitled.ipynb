{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wrapt\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/bc/7993faa8084b5a5dbabb07a197ae1b7590da4752dc80455d878573553e2f/wrapt-1.12.0.tar.gz\n",
      "Building wheels for collected packages: wrapt\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/54/f9/95/099544e9f879f719b14cf567fabb5aa7984263df0f025f3eef\n",
      "Successfully built wrapt\n",
      "Installing collected packages: wrapt\n",
      "Successfully installed wrapt-1.12.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 109.2MB 496kB/s eta 0:00:0113% |█▎                              | 4.4MB 82.0MB/s eta 0:00:02    8% |██▋                             | 8.8MB 89.8MB/s eta 0:00:02    12% |████                            | 13.3MB 85.1MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting google-pasta>=0.1.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl (57kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 44.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K    100% |████████████████████████████████| 491kB 57.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 24.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 42.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting numpy<2.0,>=1.14.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.2MB 2.6MB/s  eta 0:00:01   14% |████▊                           | 2.9MB 96.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 43.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 66.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/97/bece4417f349f8f83252232ef66ea63eb47f8044ca61b51e2a478e2c7a94/grpcio-1.27.2-cp36-cp36m-manylinux1_x86_64.whl (2.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.7MB 19.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 56.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/72/1c1498c1e908e0562b1e1cd30012580baa7d33b5b0ffdbeb5fde2462cc71/setuptools-45.2.0-py3-none-any.whl (584kB)\n",
      "\u001b[K    100% |████████████████████████████████| 593kB 53.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Building wheels for collected packages: termcolor, absl-py\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: google-pasta, tensorflow-estimator, absl-py, grpcio, numpy, setuptools, markdown, tensorboard, astor, keras-applications, gast, termcolor, keras-preprocessing, tensorflow\n",
      "  Found existing installation: numpy 1.14.3\n",
      "    Uninstalling numpy-1.14.3:\n",
      "      Successfully uninstalled numpy-1.14.3\n",
      "  Found existing installation: setuptools 39.1.0\n",
      "    Uninstalling setuptools-39.1.0:\n",
      "      Successfully uninstalled setuptools-39.1.0\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 google-pasta-0.1.8 grpcio-1.27.2 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 numpy-1.18.1 setuptools-45.2.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wrapt --upgrade --ignore-installed\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Recall, Precision\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Testing and optimization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics.regression import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# import module\n",
    "from src.pipeline import *\n",
    "\n",
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer   \n",
    "import pickle\n",
    "import datetime as dt\n",
    "import glob\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install --upgrade pip\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = 'fakenewscorpus'\n",
    "key = 'data/2M_df.pkl'\n",
    "# obj = s3.get_object(Bucket='bucket', Key='key')\n",
    "df = pickle.loads(s3.Bucket(bucket).Object(key).get()['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33273061724603664"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balance(df):\n",
    "    n_pos = len(df[df['label']==1])\n",
    "    n_neg = len(df[df['label']==0])\n",
    "    return n_pos, n_neg\n",
    "\n",
    "n_pos, n_neg = balance(df)\n",
    "\n",
    "n_pos/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33431, 66569)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = pd.read_csv('data/sw1k.csv')['term'].to_numpy()\n",
    "\n",
    "sample = df.sample(100000,axis=0)\n",
    "\n",
    "balance(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(sample['content'],sw)\n",
    "\n",
    "sample['token']=tokens\n",
    "\n",
    "X = tokens\n",
    "y = sample['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "bow, tf, tfidf, cv, tv = vectorize(X_train,max_features=5000,ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf).to_csv('data/sample_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.read_csv('data/sample_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 5000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5466"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGHT = len(max(X_train, key=len))\n",
    "MAX_SEQ_LENGHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'fsu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3f157584ab1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN_FEATURES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mvaries\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfastest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mFORTRAN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontiguous\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mvaries\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfastest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'fsu'"
     ]
    }
   ],
   "source": [
    "N_FEATURES = len(bow)\n",
    "X_train_sequences = pad_sequences(X_train, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6000, 500)         11002000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                144640    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 11,146,705\n",
      "Trainable params: 11,146,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "# model.add(Embedding(5000 + 1,\n",
    "#                     64  # Embedding size\n",
    "#                    ))\n",
    "\n",
    "model3.add(LSTM(units=64, return_sequences=True, input_shape=(6000,5000)))\n",
    "model3.add(LSTM(64),)\n",
    "model3.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC','BinaryAccuracy'])\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_temp=tfidf.reshape(1,7500,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (6000, 5000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cddda303563f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model3.fit(tfidf_temp[:-1500], y_train[:-1500], \n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           validation_data=(tfidf_temp[-1500:], y_train[-1500:]))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    374\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have 3 dimensions, but got array with shape (6000, 5000)"
     ]
    }
   ],
   "source": [
    "model3.fit(tfidf_temp[:-1500], y_train[:-1500], \n",
    "          epochs=3, batch_size=128, verbose=1,\n",
    "          validation_data=(tfidf_temp[-1500:], y_train[-1500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/sample_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('saved_model/sample_model',compile = False)\n",
    "tfidf = pd.read_csv('data/sample_tfidf.csv')\n",
    "X_train = tfidf.set_ind\n",
    "y_train = sample['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/sample_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [' '.join(row) for row in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 37s 15ms/sample - loss: 0.6377 - auc_14: 0.5000 - binary_accuracy: 0.6656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.637664122581482, 0.5, 0.6656]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 500)               2500500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 2,501,001\n",
      "Trainable params: 2,501,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    " \n",
    "model2.add(Dense(units=500, activation='relu', input_dim=5000))\n",
    "model2.add(Dense(units=1, activation='sigmoid'))\n",
    " \n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC','BinaryAccuracy','Recall','Precision'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "65000/65000 [==============================] - 4s 56us/sample - loss: 0.3315 - auc_2: 0.9143 - binary_accuracy: 0.8586 - recall: 0.6843 - precision: 0.8681 - val_loss: 0.2754 - val_auc_2: 0.9386 - val_binary_accuracy: 0.8855 - val_recall: 0.7291 - val_precision: 0.9064\n",
      "Epoch 2/30\n",
      "65000/65000 [==============================] - 3s 51us/sample - loss: 0.2491 - auc_2: 0.9514 - binary_accuracy: 0.8976 - recall: 0.7859 - precision: 0.8975 - val_loss: 0.2642 - val_auc_2: 0.9423 - val_binary_accuracy: 0.8889 - val_recall: 0.7557 - val_precision: 0.8919\n",
      "Epoch 3/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.2161 - auc_2: 0.9637 - binary_accuracy: 0.9110 - recall: 0.8133 - precision: 0.9133 - val_loss: 0.2657 - val_auc_2: 0.9431 - val_binary_accuracy: 0.8889 - val_recall: 0.7884 - val_precision: 0.8639\n",
      "Epoch 4/30\n",
      "65000/65000 [==============================] - 3s 53us/sample - loss: 0.1778 - auc_2: 0.9763 - binary_accuracy: 0.9300 - recall: 0.8503 - precision: 0.9361 - val_loss: 0.2706 - val_auc_2: 0.9440 - val_binary_accuracy: 0.8905 - val_recall: 0.7884 - val_precision: 0.8685\n",
      "Epoch 5/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.1342 - auc_2: 0.9873 - binary_accuracy: 0.9526 - recall: 0.8970 - precision: 0.9598 - val_loss: 0.2783 - val_auc_2: 0.9443 - val_binary_accuracy: 0.8901 - val_recall: 0.7963 - val_precision: 0.8611\n",
      "Epoch 6/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0886 - auc_2: 0.9950 - binary_accuracy: 0.9753 - recall: 0.9443 - precision: 0.9818 - val_loss: 0.3010 - val_auc_2: 0.9430 - val_binary_accuracy: 0.8897 - val_recall: 0.8029 - val_precision: 0.8548\n",
      "Epoch 7/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0532 - auc_2: 0.9982 - binary_accuracy: 0.9893 - recall: 0.9733 - precision: 0.9947 - val_loss: 0.3173 - val_auc_2: 0.9410 - val_binary_accuracy: 0.8927 - val_recall: 0.7947 - val_precision: 0.8697\n",
      "Epoch 8/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0310 - auc_2: 0.9994 - binary_accuracy: 0.9951 - recall: 0.9864 - precision: 0.9990 - val_loss: 0.3489 - val_auc_2: 0.9386 - val_binary_accuracy: 0.8925 - val_recall: 0.7929 - val_precision: 0.8706\n",
      "Epoch 9/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0193 - auc_2: 0.9998 - binary_accuracy: 0.9968 - recall: 0.9910 - precision: 0.9994 - val_loss: 0.3660 - val_auc_2: 0.9374 - val_binary_accuracy: 0.8912 - val_recall: 0.8023 - val_precision: 0.8595\n",
      "Epoch 10/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0133 - auc_2: 0.9999 - binary_accuracy: 0.9973 - recall: 0.9925 - precision: 0.9995 - val_loss: 0.3973 - val_auc_2: 0.9356 - val_binary_accuracy: 0.8896 - val_recall: 0.8083 - val_precision: 0.8505\n",
      "Epoch 11/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0102 - auc_2: 0.9999 - binary_accuracy: 0.9975 - recall: 0.9931 - precision: 0.9996 - val_loss: 0.4181 - val_auc_2: 0.9329 - val_binary_accuracy: 0.8899 - val_recall: 0.8077 - val_precision: 0.8518\n",
      "Epoch 12/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0086 - auc_2: 0.9999 - binary_accuracy: 0.9977 - recall: 0.9934 - precision: 0.9997 - val_loss: 0.4423 - val_auc_2: 0.9307 - val_binary_accuracy: 0.8898 - val_recall: 0.8047 - val_precision: 0.8538\n",
      "Epoch 13/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0075 - auc_2: 0.9999 - binary_accuracy: 0.9978 - recall: 0.9936 - precision: 0.9999 - val_loss: 0.4604 - val_auc_2: 0.9285 - val_binary_accuracy: 0.8904 - val_recall: 0.8056 - val_precision: 0.8547\n",
      "Epoch 14/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0070 - auc_2: 0.9999 - binary_accuracy: 0.9978 - recall: 0.9936 - precision: 0.9999 - val_loss: 0.4726 - val_auc_2: 0.9287 - val_binary_accuracy: 0.8894 - val_recall: 0.8117 - val_precision: 0.8475\n",
      "Epoch 15/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0066 - auc_2: 0.9999 - binary_accuracy: 0.9978 - recall: 0.9937 - precision: 0.9999 - val_loss: 0.4904 - val_auc_2: 0.9278 - val_binary_accuracy: 0.8886 - val_recall: 0.8096 - val_precision: 0.8469\n",
      "Epoch 16/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0063 - auc_2: 0.9999 - binary_accuracy: 0.9978 - recall: 0.9937 - precision: 0.9999 - val_loss: 0.5151 - val_auc_2: 0.9243 - val_binary_accuracy: 0.8926 - val_recall: 0.7963 - val_precision: 0.8682\n",
      "Epoch 17/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0060 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.5225 - val_auc_2: 0.9244 - val_binary_accuracy: 0.8920 - val_recall: 0.8008 - val_precision: 0.8629\n",
      "Epoch 18/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0059 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9937 - precision: 1.0000 - val_loss: 0.5355 - val_auc_2: 0.9237 - val_binary_accuracy: 0.8910 - val_recall: 0.8035 - val_precision: 0.8580\n",
      "Epoch 19/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0058 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.5525 - val_auc_2: 0.9217 - val_binary_accuracy: 0.8927 - val_recall: 0.8029 - val_precision: 0.8632\n",
      "Epoch 20/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0058 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 0.9999 - val_loss: 0.5601 - val_auc_2: 0.9224 - val_binary_accuracy: 0.8910 - val_recall: 0.8053 - val_precision: 0.8566\n",
      "Epoch 21/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0057 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.5750 - val_auc_2: 0.9205 - val_binary_accuracy: 0.8920 - val_recall: 0.8053 - val_precision: 0.8594\n",
      "Epoch 22/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0057 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.5818 - val_auc_2: 0.9196 - val_binary_accuracy: 0.8913 - val_recall: 0.8029 - val_precision: 0.8593\n",
      "Epoch 23/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0057 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.5880 - val_auc_2: 0.9200 - val_binary_accuracy: 0.8905 - val_recall: 0.8080 - val_precision: 0.8532\n",
      "Epoch 24/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0132 - auc_2: 0.9997 - binary_accuracy: 0.9954 - recall: 0.9901 - precision: 0.9963 - val_loss: 0.5385 - val_auc_2: 0.9189 - val_binary_accuracy: 0.8848 - val_recall: 0.7947 - val_precision: 0.8475\n",
      "Epoch 25/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0111 - auc_2: 0.9999 - binary_accuracy: 0.9966 - recall: 0.9920 - precision: 0.9980 - val_loss: 0.5462 - val_auc_2: 0.9218 - val_binary_accuracy: 0.8874 - val_recall: 0.8035 - val_precision: 0.8481\n",
      "Epoch 26/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0061 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9937 - precision: 1.0000 - val_loss: 0.5754 - val_auc_2: 0.9189 - val_binary_accuracy: 0.8910 - val_recall: 0.7963 - val_precision: 0.8636\n",
      "Epoch 27/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0057 - auc_2: 1.0000 - binary_accuracy: 0.9979 - recall: 0.9937 - precision: 1.0000 - val_loss: 0.5954 - val_auc_2: 0.9174 - val_binary_accuracy: 0.8913 - val_recall: 0.7956 - val_precision: 0.8649\n",
      "Epoch 28/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0057 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.6070 - val_auc_2: 0.9167 - val_binary_accuracy: 0.8917 - val_recall: 0.7975 - val_precision: 0.8646\n",
      "Epoch 29/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0056 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.6153 - val_auc_2: 0.9165 - val_binary_accuracy: 0.8913 - val_recall: 0.8005 - val_precision: 0.8611\n",
      "Epoch 30/30\n",
      "65000/65000 [==============================] - 3s 52us/sample - loss: 0.0056 - auc_2: 0.9999 - binary_accuracy: 0.9979 - recall: 0.9938 - precision: 1.0000 - val_loss: 0.6179 - val_auc_2: 0.9176 - val_binary_accuracy: 0.8897 - val_recall: 0.8050 - val_precision: 0.8533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f87905ce390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(tfidf[:-10000], y_train[:-10000], \n",
    "          epochs=30, batch_size=128, verbose=1,\n",
    "          validation_data=(tfidf[-10000:], y_train[-10000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('saved_model/sample_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 59us/sample - loss: 0.6468 - auc_2: 0.9164 - binary_accuracy: 0.8898 - recall: 0.8058 - precision: 0.8510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6467978135927022, 0.9164012, 0.88976, 0.8058158, 0.8509572]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(tfidf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score = 0.81"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
